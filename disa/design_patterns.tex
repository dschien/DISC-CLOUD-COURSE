%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Algorithm Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Developing algorithms involve:}
    \begin{itemize}
    \item Preparing the input data
    \item Implement the mapper and the reducer
    \item Optionally, design the combiner and the partitioner
   \end{itemize}

    \vspace{20pt}

  \item \textbf{How to recast existing algorithms in ``Map Reduce''?}
    \begin{itemize}
    \item It is not always obvious how to express algorithms
    \item Data structures play an important role
    \item Optimization is hard
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Learn by examples}
    \begin{itemize}
    \item ``Design patterns''
    \item ``Shuffle'' is perhaps the most tricky aspect
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Algorithm Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Aspects that are {\color{red}not} under the control of the
    designer}
    \begin{itemize}
    \item \textit{Where} a mapper or reducer will run
    \item \textit{When} a mapper or reducer begins or finishes
    \item \textit{Which} input key-value pairs are processed by a
      specific mapper
    \item \textit{Which} intermediate key-value pairs are processed by a
      specific reducer
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Aspects that can be controlled}
    \begin{itemize}
    \item Construct {\color{red}data structures as keys and values}
    \item Execute user-specified initialization and termination code
      for mappers and reducers
    \item Preserve state across multiple input and intermediate keys
      in mappers and reducers
    \item {\color{red}Control the sort order} of intermediate keys, and therefore
      the order in which a reducer will encounter particular keys
    \item {\color{red}Control the partitioning of the key space}, and therefore the
      set of keys that will be encountered by a particular reducer
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Algorithm Design}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{``Map Reduce'' algorithms can be complex}
    \begin{itemize}
    \item Hadoop MapReduce requires algorithm decomposition in several jobs
    \item Apache Spark is much simpler
    \item In general, iterative algorithms require a {\color{red} driver}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Basic design patterns\footnote{You will see them in action during the laboratory sessions.}}
    \begin{itemize}
    \item Local Aggregation
    \item Pairs and Stripes
    \item Order inversion
    \end{itemize}
 \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local Aggregation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Local Aggregation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{In the context of data-intensive distributed processing, the
      most important aspect of synchronization is the {\color{red}exchange of
        intermediate results}}
    \begin{itemize}
    \item This involves copying intermediate results from the
      processes that produced them to those that consume them
    \item In general, this involves \textbf{data transfers over the network}
    \item In Hadoop, also disk I/O is involved, as intermediate
      results are written to disk
    \end{itemize}

    \vspace{20pt}

    \item \textbf{Network and disk latencies are expensive}
      \begin{itemize}
      \item Reducing the amount of intermediate data translates into
        algorithmic efficiency
      \end{itemize}

      \vspace{20pt}

    \item \textbf{Combiners and preserving state across inputs}
      \begin{itemize}
      \item Reduce the number and size of key-value pairs to be shuffled
      \end{itemize}

  \end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{In-Mapper Combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{In-Mapper Combiners, a possible improvement over vanilla Combiners}
    \begin{itemize}
    \item Hadoop does not\footnote{Actually, combiners are not called if the number of map output records is less than a small threshold, {\it i.e.}, 4} guarantee combiners to be executed
    \item Combiners can be costly in terms of CPU and I/O
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Use a hash map to cumulate intermediate
      results}
    \begin{itemize}
    \item The data structure is also know as ``associative array'' or ``dictionary''
    \item The array is used to tally up term counts within a single ``document''
    \item The \texttt{Emit} method is called only after all \texttt{InputRecords} have been processed
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Example (see next slide)}
    \begin{itemize}
    \item The code emits a key-value pair for each {\color{red}unique}
      term in the document
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{In-Memory Combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\algrenewcommand\algorithmicfunction{\textbf{class}}
\algrenewcommand\algorithmicprocedure{\textbf{method}}

  \begin{algorithmic}[1]
    \Function{Mapper}{}
    \Procedure{Map}{offset $a$, line $l$}
    \State $H \gets$  new HashMap
    \ForAll{term $t \in$ line $l$}
      \State $H\{t\} \gets H\{t\} + 1$
    \EndFor
    \ForAll{term $t \in$ $H$}
      \State $\textsc{Emit}(\textrm{term }t, \textrm{count }H\{t\})$
    \EndFor
    \EndProcedure
    \EndFunction
  \end{algorithmic}
\end{algorithm}

}

\frame {\frametitle{In-Memory Combiners}
  \begin{itemize}
  \item \textbf{Taking the idea one step further}
    \begin{itemize}
    \item Exploit implementation details in Hadoop
    \item A Java mapper object is created for each map task
    \item JVM reuse must be enabled
    \end{itemize}

    \vspace{40pt}

  \item \textbf{Preserve state within and across calls to the \texttt{Map}
    method}
    \begin{itemize}
    \item \texttt{Initialize} method, used to create an across-map, persistent
      data structure
    \item \texttt{Close} method, used to emit intermediate key-value
      pairs only when all map task scheduled on one machine are done
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{In-Memory Combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\algrenewcommand\algorithmicfunction{\textbf{class}}
\algrenewcommand\algorithmicprocedure{\textbf{method}}

  \begin{algorithmic}[1]
    \Function{Mapper}{}
      \Procedure{Initialize}{}
        \State $H \gets$  new HashMap
      \EndProcedure
      \Procedure{Map}{offset $a$, line $l$}
        \ForAll{term $t \in$ line $l$}
          \State $H\{t\} \gets H\{t\} + 1$
        \EndFor
      \EndProcedure
      \Procedure{Close}{}
        \ForAll{term $t \in$ $H$}
          \State $\textsc{Emit}(\textrm{term }t, \textrm{count }H\{t\})$
        \EndFor
      \EndProcedure
    \EndFunction
  \end{algorithmic}
\end{algorithm}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{In-Memory Combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Summing up: a first ``design pattern'', \textit{in-memory
        combining}}
    \begin{itemize}
    \item Provides control over when local aggregation occurs
    \item Designer can determine how exactly aggregation is done
    \end{itemize}

    \vspace{40pt}

  \item \textbf{Efficiency vs. Combiners}
    \begin{itemize}
    \item There is no additional overhead due to the materialization
      of key-value pairs
      \begin{itemize}
      \item Un-necessary object creation and destruction (garbage
        collection)
      \item Serialization, deserialization when memory bounded
      \end{itemize}
    \item With combiners, mappers still need to emit all key-value pairs; combiners ``only'' reduce network traffic
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{In-Memory Combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Precautions}
    \begin{itemize}
    \item In-memory combining breaks the functional programming
      paradigm due to {\bf state preservation}
    \item Preserving state across multiple instances implies that
      algorithm behavior might depend on execution order
      \begin{itemize}
      \item Works well with commutative / associative operations
      \item Otherwise, order-dependent bugs are difficult to find
      \end{itemize}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Memory capacity is limited}
    \begin{itemize}
    \item In-memory combining strictly depends on having sufficient memory to store intermediate results
    \item A possible {\color{red}solution}: ``block'' and ``flush''
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Further Remarks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{The extent to which efficiency can be increased with local
      aggregation depends on the size of the intermediate key space}
    \begin{itemize}
    \item Opportunities for aggregation arise when multiple values
      are associated to the same keys
    \end{itemize}

    \vspace{40pt}

  \item \textbf{Local aggregation also effective to deal with reduce
      stragglers}
    \begin{itemize}
    \item Reduce the number of values associated with frequently occurring keys
    \end{itemize}
  \end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing the average, with in-mapper combiners}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
    \item Partial sums and counts are held in memory (across inputs)
    \item Intermediate values are emitted only after the entire input
      split is processed
    \item The output value is a pair
  \end{itemize}

\begin{algorithm}[H]
\algrenewcommand\algorithmicfunction{\textbf{class}}
\algrenewcommand\algorithmicprocedure{\textbf{method}}

  \begin{algorithmic}[1]
    \Function{Mapper}{}
      \Procedure{Initialize}{}
        \State $S \gets$  new HashMap
        \State $C \gets$  new HashMap
      \EndProcedure
      \Procedure{Map}{term $t$, integer $r$}
        \State $S\{t\} \gets S\{t\} + r$
        \State $C\{t\} \gets C\{t\} + 1$
      \EndProcedure
      \Procedure{Close}{}
        \ForAll{term $t \in$ $S$}
          \State $\textsc{Emit}(\textrm{term }t, \textrm{pair }(S\{t\},C\{t\}))$
        \EndFor
      \EndProcedure
    \EndFunction
  \end{algorithmic}
\end{algorithm}

} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pairs and Stripes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Pairs and Stripes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{A common approach in MapReduce: build {\color{red}complex} keys}
    \begin{itemize}
    \item Use the framework to group data together
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Two basic techniques:}
   \begin{itemize}
    \item \textit{Pairs}: similar to the example on the average
    \item \textit{Stripes}: uses in-mapper memory data structures
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Next, we focus on a particular problem that benefits
      from these two methods}
 \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{The problem: building word co-occurrence matrices for large corpora}
    \begin{itemize}
    \item The co-occurrence matrix of a corpus is a square $n \times n$ matrix, $M$
    \item $n$ is the number of unique words (\textit{i.e.}, the vocabulary size)
    \item A cell $m_{ij}$ contains the number of times the word $w_i$ co-occurs with word $w_j$ \textit{within a specific context}
    \item Context: a sentence, a paragraph a document or a window of $m$ words
    \item NOTE: the matrix may be symmetric in some cases
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Motivation}
    \begin{itemize}
    \item This problem is a basic building block for more complex operations
    \item {\color{red}Estimating the distribution of discrete joint events from a large number of observations}
    \item Similar problem in other domains:
      \begin{itemize}
      \item Customers who buy \textit{this} tend to also buy
        \textit{that}
      \end{itemize}
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Observations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Space requirements}
    \begin{itemize}
    \item Clearly, the space requirement is $O(n^2)$, where $n$ is the size of the vocabulary
    \item For real-world (English) corpora $n$ can be hundreds of thousands of words, or even billions of worlds in some specific cases
    \end{itemize}

    \vspace{20pt}

  \item \textbf{So what's the problem?}
    \begin{itemize}
    \item If the matrix can fit in the memory of a single machine, then just use whatever naive implementation
    \item Instead, if the matrix is bigger than the available memory, then {\color{red}paging} would kick in, and any naive
      implementation would break
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Compression}
    \begin{itemize}
    \item Such techniques can help in solving the problem on a single machine
    \item However, there are scalability problems
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Pairs approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{center}
    \includegraphics[scale=0.36]{./Figures/pairs}
  \end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Pairs approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Input to the problem}
    \begin{itemize}
    \item Key-value pairs in the form of a \texttt{offset} and a \texttt{line}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{The mapper:}
    \begin{itemize}
    \item Processes each input document
      \item Emits key-value pairs with:
        \begin{itemize}
        \item Each co-occurring word {\color{red}pair} as the key
        \item The integer one (the count) as the value
        \end{itemize}
      \item This is done with two nested loops:
        \begin{itemize}
        \item The outer loop iterates over all words
        \item The inner loop iterates over all neighbors
        \end{itemize}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{The reducer:}
    \begin{itemize}
    \item Receives {\color{red}pairs} related to co-occurring words
      \begin{itemize}
      \item This {\color{red}requires \textbf{modifying the partitioner}}
      \end{itemize}
    \item Computes an absolute count of the joint event
    \item Emits the pair and the count as the final key-value output
      \begin{itemize}
      \item Basically reducers emit the cells of the output matrix
      \end{itemize}
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Pairs approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\algrenewcommand\algorithmicfunction{\textbf{class}}
\algrenewcommand\algorithmicprocedure{\textbf{method}}

  \begin{algorithmic}[1]
    \Function{Mapper}{}
      \Procedure{Map}{offset $a$, line $l$}
        \ForAll{term $w \in$ line $l$}
          \ForAll{term $u \in \textsc{Neighbors}(w)$}
            \State $\textsc{Emit } (\textrm{pair }(w,u), \textrm{count }1)$
          \EndFor
        \EndFor
      \EndProcedure
    \EndFunction

    \Function{Reducer}{}
      \Procedure{Reduce}{pair $p$, counts $[c_1,c_2, \cdots ]$}
        \State $s \gets 0$
        \ForAll{count $c \in \textrm{counts }[c_1,c_2, \cdots ]$}
          \State $s \gets s + c$
        \EndFor
        \State $\textsc{Emit } (\textrm{pair }p, \textrm{count }s)$
      \EndProcedure
    \EndFunction

  \end{algorithmic}
\end{algorithm}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Stripes approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{center}
    \includegraphics[scale=0.36]{./Figures/stripes}
  \end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Stripes approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Input to the problem}
    \begin{itemize}
    \item Key-value pairs in the form of a \texttt{offset} and a \texttt{line}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{The mapper:}
    \begin{itemize}
    \item Same two nested loops structure as before
    \item Co-occurrence information is first stored in an associative
      array
    \item Emit key-value pairs with {\color{red}words} as keys and the
      corresponding arrays as values
    \end{itemize}

    \vspace{20pt}

  \item \textbf{The reducer:}
    \begin{itemize}
    \item Receives all hash maps related to the same word
    \item Performs an element-wise sum of all hash maps with
      the same key
    \item Emits key-value output in the form of word, hash map
      \begin{itemize}
      \item Basically, reducers emit \textbf{rows} of the co-occurrence matrix
      \end{itemize}
    \end{itemize}
 \end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Word co-occurrence: the Stripes approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[H]
\algrenewcommand\algorithmicfunction{\textbf{class}}
\algrenewcommand\algorithmicprocedure{\textbf{method}}

  \begin{algorithmic}[1]
    \Function{Mapper}{}
      \Procedure{Map}{offset $a$, line $l$}
        \ForAll{term $w \in$ line $l$}
        \State $H \gets$ new HashMap
          \ForAll{term $u \in \textsc{Neighbors}(w)$}
            \State $H\{u\} \gets H\{u\}+1$
          \EndFor
          \State $\textsc{Emit } (\textrm{term }w, \textrm{Stripe }H)$
        \EndFor
      \EndProcedure
    \EndFunction

    \Function{Reducer}{}
      \Procedure{Reduce}{term $w$, Stripes $[H_1,H_2,H_3 \cdots ]$}
        \State $H_f \gets$ new HashMap
        \ForAll{Stripe $H \in \textrm{Stripes }[H_1,H_2,H_3 \cdots ]$}
          \State $\textsc{Sum}(H_f,H)$
        \EndFor
        \State $\textsc{Emit } (\textrm{term }w, \textrm{Stripe }H_f)$
      \EndProcedure
    \EndFunction

  \end{algorithmic}
\end{algorithm}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Pairs and Stripes, a comparison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{The pairs approach}
    \begin{itemize}
    \item Generates a large number of key-value pairs
    \begin{itemize}
      \item In particular, intermediate ones, that fly over the network
    \end{itemize}
    \item The benefit from combiners is limited, as it is less likely
      for a mapper to process multiple occurrences of a word
    \item Does not suffer from memory paging problems
    \end{itemize}

    \vspace{20pt}
    
  \item \textbf{The stripes approach}
    \begin{itemize}
    \item More compact
    \item Generates fewer and shorted intermediate keys
      \begin{itemize}
      \item The framework has less sorting to do
      \end{itemize}
    \item The values are more complex and have serialization / deserialization overhead
    \item Greatly benefits from combiners, as the key space is the vocabulary
    \item Suffers from memory paging problems, if not properly engineered
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Order Inversion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{``Relative'' Co-occurrence matrix construction}
    \begin{itemize}
    \item Similar problem as before, same matrix
    \item Instead of absolute counts, we take into consideration the
      fact that some words appear more frequently than others
      \begin{itemize}
      \item Word $w_i$ may co-occur frequently with word $w_j$ simply
        because one of the two is very common
      \end{itemize}
    \item We need to convert absolute counts to relative frequencies
      $f(w_j | w_i)$
      \begin{itemize}
      \item What proportion of the time does $w_j$ appear in the
        context of $w_i$?
      \end{itemize}
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Formally, we compute:}
    $$ f(w_j | w_i) = \frac{N(w_i,w_j)}{\sum_{w'} N(w_i, w')}$$
    \begin{itemize}
    \item $N(\cdot,\cdot)$ is the number of times a co-occurring word
      pair is observed
    \item The denominator is called the marginal
    \end{itemize}


  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{The stripes approach}
    \begin{itemize}
    \item In the reducer, the counts of all words that co-occur with
      the conditioning variable ($w_i$) are available in the
      hash map
    \item Hence, the sum of all those counts gives the marginal
    \item Then we divide the joint counts by the marginal and
      we're done
    \end{itemize}

    \vspace{40pt}

  \item \textbf{The pairs approach}
    \begin{itemize}
    \item The reducer receives the pair $(w_i,w_j)$ and the count
    \item From this information alone \textbf{it is not possible} to compute
      $f(w_j | w_i)$
    \item Fortunately, as for the mapper, also the reducer can
      {\color{red}preserve state} across multiple keys
      \begin{itemize}
      \item We can buffer in memory all the words that co-occur with
        $w_i$ and their counts
      \item This is basically building the hash map in the
        stripes method
      \end{itemize}
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies: a basic approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{We must define the sort order of the pair}
    \begin{itemize}
    \item In this way, the keys are first sorted by the left word, and
      then by the right word (in the pair)
    \item Hence, we can detect if all pairs associated with the word
      we are conditioning on ($w_i$) have been seen
    \item At this point, we can use the in-memory buffer, compute the
      relative frequencies and emit
    \end{itemize}

    \vspace{20pt}

  \item \textbf{We must define an appropriate partitioner}
    \begin{itemize}
    \item The default partitioner is based on the hash value of the
      intermediate key, modulo the number of reducers
    \item For a complex key, the {\bf raw byte representation} is used to
      compute the hash value
      \begin{itemize}
      \item Hence, there is no guarantee that the pair (dog, aardvark)
        and (dog,zebra) are sent to the same reducer
      \end{itemize}
    \item What we want is that all pairs with the same left word are
      sent to the same reducer
   \end{itemize}

    \vspace{20pt}

  \item \textbf{Limitations of this approach}
    \begin{itemize}
    \item Essentially, we reproduce the stripes method in the reducer
      and we need to use a custom partitioner
    \item This algorithm would work, but present the same
      memory-bottleneck problem as the stripes method
    \end{itemize}
  \end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies: order inversion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{The key is to properly sequence data presented to
      reducers}
    \begin{itemize}
    \item If it were possible to compute the marginal in the reducer
      before processing the joint counts, the reducer could simply
      divide the joint counts received from mappers by the marginal
    \item The notion of ``before'' and ``after'' can be captured in
      the {\color{red}ordering of key-value pairs}
    \item The programmer can define the sort order of keys so that
      data needed earlier is presented to the reducer before data that
      is needed later
    \end{itemize}
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies: order inversion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Recall that mappers emit pairs of co-occurring words
      as keys}

    \vspace{20pt}

  \item \textbf{The mapper:}
    \begin{itemize}
    \item additionally emits a ``special'' key of the form $(w_i,*)$
    \item The value associated to the special key is one, that
      represents the contribution of the word pair to the marginal
    \item Using combiners, these partial marginal counts will be
      aggregated before being sent to the reducers
    \end{itemize}

    \vspace{20pt}

  \item \textbf{The reducer:}
    \begin{itemize}
    \item We must make sure that the special key-value pairs are
      processed {\color{red}before} any other key-value pairs where
      the left word is $w_i$
    \item We also need to modify the partitioner as before,
      \textit{i.e.}, it would take into account only the first word
    \end{itemize}
  \end{itemize}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame {\frametitle{Computing relative frequencies: order inversion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{itemize}
  \item \textbf{Memory requirements:}
    \begin{itemize}
    \item Minimal, because only the marginal (an integer) needs to be
      stored
    \item No buffering of individual co-occurring word
    \item No scalability bottleneck
    \end{itemize}

    \vspace{20pt}

  \item \textbf{Key ingredients for order inversion}
    \begin{itemize}
    \item Emit a special key-value pair to capture the marginal
    \item Control the sort order of the intermediate key, so that the
      special key-value pair is processed first
    \item Define a custom partitioner for routing intermediate
      key-value pairs
    \item Preserve state across multiple keys in the reducer
    \end{itemize}
  \end{itemize}
}
